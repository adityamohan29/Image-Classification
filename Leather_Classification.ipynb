{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guidelines before running the code\n",
    "\n",
    "#Folder Desciriptions:-\n",
    "\n",
    "# My trianing data is in the folder Desktop\\LIMG\\Traintest\\Training\n",
    "# Validation Data is in Desktop\\Test\n",
    "# Create two files called data.h5 and labels.h5 in Desktop\\LIMG\\Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "import pandas as pd\n",
    "import mahotas\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing some important variables\n",
    "\n",
    "fixed_size = tuple((100,100))\n",
    "#initializing training path\n",
    "train_path = \"Desktop\\LIMG\\Traintest\\Training\"\n",
    "num_trees = 100\n",
    "bins = 8\n",
    "test_size =0.10\n",
    "seed = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature no:1. Extracting Shape Features\n",
    "\n",
    "def fd_hu_moments(image):\n",
    "    #converting to greyscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature no:2. Extracting Greyscale Features\n",
    "\n",
    "def fd_haralick(image):\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    return haralick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature no:3. Extracting Colour Features\n",
    "\n",
    "def fd_histogram(image, mask=None):\n",
    "   \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the two training labels( folders )\n",
    "\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "#train_labels.sort()\n",
    "print(train_labels)\n",
    "\n",
    "global_features = []\n",
    "labels = []\n",
    "\n",
    "images_per_class = 10890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for training_name in train_labels:\n",
    "   \n",
    "    dir = os.path.join(train_path, training_name)\n",
    "\n",
    "    current_label = training_name\n",
    "    \n",
    "    \n",
    "    #going through each photo and extracting the three features\n",
    "    for x in range(1,images_per_class+1):\n",
    "        \n",
    "        #each photo is made in the format dir\\(x).jpg where x is the number of the image\n",
    "        file = dir + \"\\(\" + str(x) + \").jpg\"\n",
    "        image = cv2.imread(file)\n",
    "        image = cv2.resize(image, fixed_size)\n",
    "        \n",
    "        fv_hu_moments = fd_hu_moments(image)\n",
    "        fv_haralick   = fd_haralick(image)\n",
    "        fv_histogram  = fd_histogram(image)\n",
    "\n",
    "        #stacking these three features into a variable called global_feature\n",
    "        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "\n",
    "        labels.append(current_label)\n",
    "        global_features.append(global_feature)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "targetNames = np.unique(labels)\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(labels)\n",
    "\n",
    "\n",
    "# normalize the feature vector in the range (0-1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaled_features = scaler.fit_transform(global_features)\n",
    "\n",
    "\n",
    "# save the feature vector using HDF5\n",
    "h5f_data = h5py.File('Desktop\\\\LIMG\\\\Output\\\\data.h5', 'w')\n",
    "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
    "\n",
    "h5f_label = h5py.File('Desktop\\\\LIMG\\\\Output\\\\labels.h5', 'w')\n",
    "h5f_label.create_dataset('dataset_1', data=np.array(target))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary packages for classification\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "scoring = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of using HDF5 file-format, we could use “.csv” file-format to store the features.\n",
    "#But, as we will be working with large amounts of data in future, HDF5 format will be better.\n",
    "\n",
    "h5f_data = h5py.File('Desktop\\LIMG\\Output\\data.h5', 'r')\n",
    "h5f_label = h5py.File('Desktop\\LIMG\\Output\\labels.h5', 'r')\n",
    "\n",
    "global_features_string = h5f_data['dataset_1']\n",
    "global_labels_string = h5f_label['dataset_1']\n",
    "\n",
    "global_features = np.array(global_features_string)\n",
    "global_labels = np.array(global_labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#we will use train_test_split function provided by scikit-learn to split our training dataset into train_data and test_data. \n",
    "#By this way, we train the models with the train_data and\n",
    "#test the trained model with the unseen test_data. The split size is decided by the test_size parameter.\n",
    "\n",
    "\n",
    "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features), np.array(global_labels), test_size=test_size, random_state=seed)\n",
    "                                                                                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=9)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# filter all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# go through random forest alggorithm and print accuracy\n",
    "#more models can be added in the models[] variable to compare accuracy\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clf  = RandomForestClassifier(n_estimators=100, random_state=9)\n",
    "\n",
    "\n",
    "clf.fit(trainDataGlobal, trainLabelsGlobal)\n",
    "\n",
    "# path to validation data\n",
    "test_path = \"Desktop\\Test\"\n",
    "\n",
    "#extract features of each validation image\n",
    "for file in glob.glob(test_path + \"\\*.jpg\"):\n",
    "  \n",
    "    image = cv2.imread(file)\n",
    "\n",
    "\n",
    "    image = cv2.resize(image, fixed_size)\n",
    "\n",
    "\n",
    "    fv_hu_moments = fd_hu_moments(image)\n",
    "    fv_haralick   = fd_haralick(image)\n",
    "    fv_histogram  = fd_histogram(image)\n",
    "\n",
    "\n",
    "    global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "\n",
    "    #predict output using clf.fit()\n",
    "    prediction = clf.predict(global_feature.reshape(1,-1))[0]\n",
    "\n",
    "    print(\"File Name: \"+file)\n",
    "    #output prediction\n",
    "    print(train_labels[prediction], \":\")\n",
    "    image = cv2.resize(image, (2000,2000))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    print(\"\\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
